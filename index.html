<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?">
  <meta name="keywords" content="OSS">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PCL-Fetcher
  </title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="static/images/icon.png">

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>


<style>
  ol li {
  margin-bottom: 1em; 
  
}

  a {
    text-decoration: none;
  }

  body {
    font-size: 18px;
  }

  table.custom-border {
    /* border-collapse: collapse;
    width: 100%; */
    border-top: 2px solid black;
    border-bottom: 2px solid black;
  }
  /* table.custom-border td, table.custom-border th {
    border: none;
  } */

</style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://decallab.cs.ucdavis.edu/" target="_blank">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Previously Built Tools
        </a>
        <div class="navbar-dropdown">
          
          <a class="navbar-item" href="https://ossreacts.netlify.app/" target="_blank">
            ReACTive
          </a>
          <a class="navbar-item" href="https://github.com/Nafiz43/EvidenceBot" target="_blank">
            EvidenceBot
          </a>
        </div>
      </div> -->


      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">  
          <a class="navbar-item" href="https://amia.secure-platform.com/cic/gallery/rounds/82015/details/17117" target="_blank">
            Automation of PCL
          </a>

          <!-- <a class="navbar-item" href="https://dl.acm.org/doi/abs/10.1145/3663529.3663777" target="_blank">
            Researched ACTionable (ReACT)
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/documentation_burden.png" alt="" style="height: 42%; width: 45%;">
          <h1 class="title is-1 publication-title">Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://nafiz43.github.io/portfolio/">Nafiz Imtiaz Khan<sup>1</sup></a>, </span>
            <span class="author-block">
              <a href="https://www.cs.ucdavis.edu/~filkov/" target="_blank">Vladimir Filkov<sup>1</sup></a>, 

            </span>
            <span class="author-block">
              <a href="" target="_blank">Kylie Cleland<sup>2</sup></a>, 
            </span>

            <span class="author-block">
              <a href="https://health.ucdavis.edu/vascular/team/42868/roger-goldman-vascular-and-interventional-radiology-sacramento" target="_blank">Roger Eric Goldman<sup>2</sup></a> 
            </span>
           
          </div>

          <div class="is-size-5 publication-authors">
            <sup>1</sup> <span class="author-block">Department of Computer Science, University of California, Davis, CA, US</span> <br>
            <sup>2</sup> <span class="author-block">Department of Radiology, University of California, Davis, CA, US</span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Live Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1Jy0-V-84rJn07zEMCqw0YxO2zHxhOxmi/view?usp=drive_link" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-alt"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>


              <span class="link-block">
                <a href="#methodology" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-cogs"></i>
                  </span>
                  <span>Methodology</span>
                </a>
              </span>


              <span class="link-block">
                <a href="#results" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-chart-line	"></i>
                  </span>
                  <span>Result</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Nafiz43/PCL-Fetcher" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://zenodo.org/records/15307373" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </span> -->
              <!-- Installation Manual. -->
            <span class="link-block">
              <a href="#Installation" target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-tools"></i>
                </span>
                <span>Install</span>
                </a>
          </span>

          <span class="link-block">
            <a href="#Docker" target="_blank"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-docker"></i>
              </span>
              <span>Docker</span>
              </a>
        </span>



          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Accurate documentation of procedural case logs is a critical component of radiology training, serving as the basis for competency assessment, credentialing, and regulatory compliance. However, current case logging workflows rely heavily on manual annotation by trainees—an error-prone and time-consuming process that imposes significant administrative burden and detracts from clinical learning. Traditional approaches, such as the crosswalk method that maps structured metadata to procedure labels, often fail to capture procedures documented only in free-text, resulting in incomplete logs and underreporting.
          </p>
          <p>
            In this study, we explore the feasibility of automating procedural case log generation using large language models (LLMs). We evaluate two state-of-the-art LLMs <code> Qwen-2.5</code> (a local, open-source model) and <code>Claude-3.5-Haiku</code> (a commercial API model)—under two prompting strategies: Instruction Prompting and Chain-of-Thought reasoning. Across 39 radiological procedures, both models substantially outperform the crosswalk benchmark in terms of sensitivity and F1-score, while maintaining high specificity. Error analysis reveals that model performance varies by procedure complexity and linguistic ambiguity. Additionally, we quantify the cost of inference in terms of latency and token generation, and demonstrate that LLM-based solutions could save over 30-40 hours of manual annotation time per resident annually. Our findings suggest that LLMs offer a scalable, cost-effective, and high-fidelity alternative to manual case logging, paving the way for intelligent documentation systems that support medical education while reducing clerical workload.
          </p>

           <!-- <p>To learn more about OSSPREY, visit our live link at <a href="https://ossprey.netlify.app" target="_blank">ossprey.netlify.app</a>.
         -->
        </div>
      </div>
    </div>
  </div>
</section>



 <!-- <section class="section" id="System-Architecture">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Installation Manual</h2>
        <div class="content has-text-justified">

        </div>
      </div>
    </div>
  </div>
</section>  -->
<section class="section" id="methodology">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">

          <p>
            Our methodology integrates a structured pipeline for automating procedural case logs using large language models (LLMs). The architecture below illustrates the complete system—from unstructured radiology report ingestion to structured output generation and evaluation.
          </p>

          <p class="has-text-centered">
            <a href="static/images/methodology.png" target="_blank">
              <img src="static/images/methodology.png" alt="Methodology Architecture Diagram" style="max-width: 100%; border: 1px solid #ccc; padding: 4px;">
            </a>
          </p>

          <p>
            The process begins with a collection of <strong>radiology reports</strong> authored by residents during clinical practice. These reports describe diagnostic and interventional procedures using unstructured narrative text.
          </p>

          <p>
            A <strong>data cleaning module</strong> is applied to each report to eliminate administrative blocks, signatures, disclaimers, and non-ASCII artifacts. The result is a <em>cleaned report</em> which is used in two parallel downstream tasks:
          </p>

          <ul>
            <li><strong>Manual Annotation:</strong> Expert annotators read the reports and label them for 39 predefined procedures. These form the <em>ground truth</em> labels.</li>
            <li><strong>LLM-Based Inference:</strong> The cleaned reports are processed by instruction-tuned models (e.g., Qwen-2.5, Claude-3.5-Haiku) using two prompt strategies—Instruction Prompting (IP) and Chain-of-Thought (CoT).</li>
          </ul>

          <p>
            The LLMs receive one prompt per procedure and return a structured JSON response with a binary label and a short justification.
          </p>

          <h3 class="title is-4">Prompting Strategies</h3>
          <p>
            The following templates guide how the LLMs are queried. Each prompt strictly enforces a JSON-based output schema to ensure consistency and downstream parsability.
          </p>

          <!-- Instruction Prompting Template -->
          <figure class="box">
            <figcaption class="has-text-weight-bold mb-2">Prompt Template for Instruction Prompting (IP)</figcaption>
<div style="text-align: left; overflow-x: hidden;">
  <pre style="white-space: pre-wrap; word-wrap: break-word;">
    <code class="language-python">
I will provide you with a radiology report, followed by several questions about it. Your task is to determine whether a specific radiology study or procedure was performed. Please follow these strict formatting guidelines for your response:

Output must be in valid JSON format with the following keys:

{
  "reason_for_the_label": "A string explaining the reasoning behind the classification.",
  "label": 1 or 0
}

Labeling criteria:
- Return 1 if the radiology study or procedure was explicitly mentioned as performed.
- Return 0 if the study or procedure was not performed, not documented, or uncertain in the report.
- Do not include any additional text or explanations outside the JSON response.
- Ensure strict adherence to this format for every response.
    </code>
  </pre>
</div>
          </figure>

          <!-- CoT Prompting Template -->
          <figure class="box">
            <figcaption class="has-text-weight-bold mb-2">
              Prompt Template for Chain-of-Thought (CoT) Prompting
            </figcaption>
            <div style="text-align: left; overflow-x: hidden;">
  <pre style="white-space: pre-wrap; word-wrap: break-word;">
    <code class="language-python">
I will provide you with a radiology report, followed by a question about whether a specific radiology study or procedure was performed.

&lt;procedure_specific_question&gt;

### Strict Output Format
Your response must be a valid JSON object with the following keys:
{
  "reason_for_the_label": "A concise explanation justifying the classification.",
  "label": 1 or 0
}
    </code>
  </pre>
</div>
          </figure>

          <p>
            After inference, each LLM response is compared to the expert-annotated ground truth using the following metrics:
          </p>

          <ul>
            <li><strong>Sensitivity:</strong> Ability to correctly detect procedures present in the report.</li>
            <li><strong>Specificity:</strong> Ability to correctly ignore procedures not present.</li>
            <li><strong>F1-Score:</strong> Harmonic mean of precision and recall.</li>
            <li><strong>Inference Time:</strong> Latency per question-response cycle.</li>
            <li><strong>Token Count:</strong> Proxy for verbosity and computational cost.</li>
          </ul>

          <p>
            These evaluations reveal that LLMs significantly outperform rule-based baselines (e.g., crosswalk-based detection) in terms of accuracy and sensitivity, while maintaining cost-effectiveness in both local and commercial deployments.
          </p>

          <p>
            The system is highly modular—supporting plug-and-play for new models, prompt types, or evaluation criteria. All LLM interactions are logged for transparency, auditability, and reproducibility, making the system viable for real-world deployment in clinical residency settings.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>


 <section class="section" id="results">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">

          <table  class="custom-border table table-striped table-hover">
  <caption>Performance Comparison of Models Across Modalities</caption>
  <thead>
    <tr>
      <th>Type</th>
      <th>Model</th>
      <th>Prompting Method</th>
      <th>Modality</th>
      <th>TP</th>
      <th>TN</th>
      <th>FP</th>
      <th>FN</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>F1Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="4">Benchmark</td>
      <td rowspan="4">Cross-Walk</td>
      <td rowspan="4">NA</td>
      <td>All</td>
      <td>451</td><td>15364</td><td>93</td><td>238</td><td>65.46</td><td>99.40</td><td><strong>73.15</strong></td>
    </tr>
    <tr>
      <td>VascularDiagonsis</td><td>143</td><td>3065</td><td>23</td><td>81</td><td>63.84</td><td>99.26</td><td>73.33</td>
    </tr>
    <tr>
      <td>VascularIntervention</td><td>157</td><td>5906</td><td>38</td><td>109</td><td>59.02</td><td>99.36</td><td>68.11</td>
    </tr>
    <tr>
      <td>NonVascularIntervention</td><td>151</td><td>6393</td><td>32</td><td>48</td><td>75.88</td><td>99.50</td><td>79.06</td>
    </tr>

    <tr>
      <td rowspan="8">Local</td>
      <td rowspan="8">Qwen-2.5:72B</td>
      <td rowspan="4">IP</td>
      <td>All</td><td>649</td><td>15174</td><td>283</td><td>40</td><td>94.19</td><td>98.17</td><td><strong>80.08</strong></td>
    </tr>
    <tr>
      <td>VascularDiagnosis</td><td>219</td><td>3068</td><td>20</td><td>5</td><td>97.77</td><td>99.35</td><td>94.60</td>
    </tr>
    <tr>
      <td>VascularIntervention</td><td>247</td><td>5803</td><td>141</td><td>19</td><td>92.86</td><td>97.63</td><td>75.54</td>
    </tr>
    <tr>
      <td>NonVascularIntervention</td><td>183</td><td>6303</td><td>122</td><td>16</td><td>91.96</td><td>98.10</td><td>72.62</td>
    </tr>

    <tr>
      <td rowspan="4">CoT</td>
      <td>All</td><td>627</td><td>15326</td><td>131</td><td>62</td><td>91.00</td><td>99.15</td><td><strong>86.66</strong></td>
    </tr>
    <tr>
      <td>VascularDiagnosis</td><td>214</td><td>3071</td><td>17</td><td>10</td><td>95.54</td><td>99.45</td><td>94.07</td>
    </tr>
    <tr>
      <td>VascularIntervention</td><td>242</td><td>5868</td><td>76</td><td>24</td><td>90.98</td><td>98.72</td><td>82.88</td>
    </tr>
    <tr>
      <td>NonVascularIntervention</td><td>171</td><td>6387</td><td>38</td><td>28</td><td>85.93</td><td>99.41</td><td>83.82</td>
    </tr>


    <tr>
      <td rowspan="8">Commercial</td>
      <td rowspan="8">Claude-3.5-Haiku</td>
      <td rowspan="4">IP</td>
      <td>All</td><td>633</td><td>14961</td><td>496</td><td>56</td><td>91.87</td><td>96.79</td><td><strong>69.64</strong></td>
    </tr>
    <tr>
      <td>VascularDiagnosis</td><td>215</td><td>3067</td><td>21</td><td>9</td><td>95.98</td><td>99.32</td><td>93.48</td>
    </tr>
    <tr>
      <td>VascularIntervention</td><td>230</td><td>5737</td><td>207</td><td>36</td><td>86.47</td><td>96.52</td><td>65.43</td>
    </tr>
    <tr>
      <td>NonVascularIntervention</td><td>188</td><td>6157</td><td>268</td><td>11</td><td>94.47</td><td>95.83</td><td>57.41</td>
    </tr>

    <tr>
      <td rowspan="4">CoT</td>

      <td>All</td><td>613</td><td>15348</td><td>109</td><td>76</td><td>88.97</td><td>99.29</td><td><strong>86.89</strong></td>
    </tr>
    <tr>
      <td>VascularDiagnosis</td><td>210</td><td>3069</td><td>19</td><td>14</td><td>93.75</td><td>99.38</td><td>92.71</td>
    </tr>
    <tr>
      <td>VascularIntervention</td><td>228</td><td>5905</td><td>39</td><td>38</td><td>85.71</td><td>99.34</td><td>85.55</td>
    </tr>
    <tr>
      <td>NonVascularIntervention</td><td>175</td><td>6374</td><td>51</td><td>24</td><td>87.94</td><td>99.21</td><td>82.35</td>
    </tr>


  </tbody>
</table>
          <p>
            The table above summarizes the performance of different models across various modalities. The results indicate that both local and commercial LLMs significantly outperform the benchmark crosswalk method, particularly in terms of sensitivity and F1-score. The choice of prompting strategy also plays a crucial role in model performance, with Chain-of-Thought prompting generally yielding better results.
          </p>

          <p>
            For a more detailed analysis, including error types and model-specific insights, please refer to the full paper.
          </p>

        </div>
      </div>
    </div>
  </div>
</section> 

<section class="section" id="Installation">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Installation Manual</h2>
        <div class="content has-text-justified">


          <h2 id="system-requirements">System Requirements</h2>
          <p>
            The system needs a GPU to get a faster response from the models. The amount of VRAM required depends on the model that we want to run. Here is an estimate:
          </p>
          <ul>
            <li>7B model requires ~4 GB</li>
            <li>13B model requires ~8 GB</li>
            <li>30B model needs ~16 GB</li>
            <li>65B model needs ~32 GB</li>
          </ul>

          <h2 id="installation">Installation</h2>
          <ol>
            <li>Clone the repository:<br><code>git clone https://github.com/Nafiz43/PCL-Fetcher </code>
            <li>Make sure Miniconda is installed. You can follow the official installation guide here: <a href="https://www.anaconda.com/docs/getting-started/miniconda/install" target="_blank">Miniconda Installation Guide</a>.</li>
            <li>Create a conda environment with Python 3.12:<br><code>conda create -n pcl-fetcher python=3.10</code></li>
            <li>Activate the conda environment:<br><code>conda activate pcl-fetcher</code></li>
            <li>Install all required packages:<br><code>pip install -r requirements.txt</code></li>
            <li>Install Ollama:<br>Visit <a href="https://ollama.com/">https://ollama.com/</a> and follow the installation instructions.</li>
            <li>Download the Ollama models you intend to use locally.</li>
          </ol>

          <h2 id="running-the-project">Running the Project</h2>
          <ol>
            <li>Place your annotated ground-truth dataset inside the <code>data</code> directory.</li>
            <li>Rename the dataset as <code>ground-truth.csv</code> to maintain consistency.</li>
            <li>Run the following command to derive responses from LLMs:<br>
              <pre><code>python3 01_run_llm.py --model_name=MODEL-NAME --prompting_method=PROMPTING-METHOD --reports_to_process=-1</code></pre>
              <strong>Command breakdown:</strong>
              <ul>
                <li><code>--model_name</code>: Name of the model to run (e.g., mixtral:8x7b-instruct-v0.1-q4_K_M)</li>
                <li><code>--prompting_method</code>: Name of the prompting method. Two options available: 1) IP, stands for Instruction Prompting; 2) CoT, stands for Chain-of-Thought prompting.</li>
                <li><code>--reports_to_process=-1</code>: Number of reports to process. By default = -1, processes all reports.</li>
              </ul>
              LLM responses will be stored in the <code>local_chat_history</code> directory.
            </li>
          </ol>

          <h2 id="calculating-the-evaluation-metrics">Calculating the Evaluation Metrics</h2>
          <ol>
            <li>Open the <code>run_evaluation.py</code> file.</li>
            <li>Update the value of the variable:<br>
              <code>file_containing_ground_truth</code> – should point to ground truth data (e.g., <code>'data/PCL_p.csv'</code>).
            </li>
            <li>Run the following command in the shell:<br>
              <pre><code>python3 run_evaluation.py --reports_to_process=-1</code></pre>
              <strong>Command breakdown:</strong>
              <ul>
                <li><code>--reports_to_process=-1</code>: Number of reports to process. Default = -1 to process all reports.</li>
              </ul>
            </li>
            <li>Results will be stored in the <code>results/all_models.csv</code> file.</li>
          </ol>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Docker">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Docker Setup Guide</h2>
        <div class="content has-text-justified">

          <h2>Docker Commands</h2>
          <ol>
            <li>
              <strong>Download the Dockerfile:</strong><br>
              Get it from <a href="https://github.com/Nafiz43/PCL-Fetcher/blob/master/Dockerfile" target="_blank">this GitHub link</a>.
            </li>

            <li>
              <strong>Build the Docker image:</strong><br>
              <pre><code>sudo docker build -t pcl-container .</code></pre>
              This will create a Docker image named <code>pcl-container</code>.
            </li>

            <li>
              <strong>Run the Docker container with GPU access:</strong><br>
              <pre><code>sudo docker run -it --rm --gpus=all pcl-container /bin/bash</code></pre>
              This opens an interactive shell in the container and enables GPU access.
            </li>

            <li>
              <strong>Start the Ollama server inside the container:</strong><br>
              <pre><code>ollama serve &</code></pre>
              This will run the Ollama server in the background.
            </li>

            <li>
              <strong>Install and launch your desired LLM model:</strong><br>
              For example, to install and run <code>llama3.2:latest</code>, run:
              <pre><code>ollama run llama3.2:latest</code></pre>
              You can substitute this with any other model you wish to test. Once the model is installed and the server is up, you can run the same experiment commands used in the non-Docker version—no additional installation needed.
            </li>

            <li>
              <strong>Copy results from the container to your local machine:</strong>
              <ol type="I">
                <li>Open a new terminal.</li>
                <li>Check the name of the running container:
                  <pre><code>sudo docker ps</code></pre>
                </li>
                <li>Copy the <code>results</code> directory using:
                  <pre><code>docker cp CONTAINER_NAME:/app/results /home/nikhan/Data/Case_Log_Data/Procedural-Case-Log</code></pre>
                  Replace <code>CONTAINER_NAME</code> with the actual container name from the previous step.
                </li>
              </ol>
            </li>

            <li>
              <strong>Exit the Docker container:</strong><br>
              <pre><code>exit</code></pre>
            </li>
          </ol>

        </div>
      </div>
    </div>
  </div>
</section>



<!-- <section class="section" id="acknowledgements">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">
          Acknowledgements
        </h2>
        <p>
          This research was supported by the National Science Foundation under Grant No. 2020751.
        </p> 
       
      </div>
    </div>
  </div>
</section> -->

<section class="section" id="License">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">
          License
        </h2>
        <p>
          The project is licensed under the Apache License 2.0. 
          This permissive license allows you to use, modify, and distribute the software for both personal and commercial purposes, 
          as long as you include proper attribution and comply with the terms outlined in the license.
        </p>
      </div>
    </div>
  </div>






<section class="section" id="Contributing">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">
          Contributing
        </h2>
        <p>
          Contributions are very welcome! If you'd like to add features, fix bugs, or improve the documentation, please feel free to fork the repository and create a pull request. Make sure your changes are well-documented and follow the project's coding standards.
        </p>
        <p>
          We appreciate your interest in improving this project—thank you for helping make it better!
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section" id="Contact">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-left">
        <h2 class="title is-3 has-text-centered">
          Contact
        </h2>
        <p>
          For high-level discussions, funding opportunities, or collaboration inquiries, please reach out to the project supervisor, 
          <strong>Professor Vladimir Filkov</strong> 
          (<a href="mailto:vfilkov@ucdavis.edu">vfilkov@ucdavis.edu</a>).
        </p>
        <p>
          For technical questions, bug reports, or concerns regarding the codebase, please contact the main project maintainer, 
          <strong>Nafiz Imtiaz Khan</strong> 
          (<a href="mailto:nikhan@ucdavis.edu">nikhan@ucdavis.edu</a>).
        </p>
        <p>
          We're excited to hear from you!
        </p>
      </div>
    </div>
  </div>
</section>


<!-- 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {},
  title     = {},
  journal   = {},
  year      = {},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      
      <a class="icon-link" href="https://github.com/nafiz43" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p class="is-size-7 has-text-grey">
            This front-end website is adapted from the 
            <a href="https://nerfies.github.io/" target="_blank" rel="noopener noreferrer">Nerfies project</a> 
            by Park et al., and is licensed under a 
            <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank" rel="noopener noreferrer">
              Creative Commons Attribution-ShareAlike 4.0 International License
            </a>. 
            It has been significantly modified and extended to fit the needs of PCL-Fetcher project.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
